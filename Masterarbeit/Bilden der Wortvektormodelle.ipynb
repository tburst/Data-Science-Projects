{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.utils import resample\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Bundestagsreden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hilfsfunktion, die aus den einzelnen xml-Dateien die Wahlperiode, die Sitzungsnummer und das Datum herausfiltert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_data(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    wahlperiode = root.find('teiHeader').find(\"fileDesc\").find(\"titleStmt\").find(\"legislativePeriod\").text\n",
    "    session = root.find('teiHeader').find(\"fileDesc\").find(\"titleStmt\").find(\"sessionNo\").text\n",
    "    date = root.find('teiHeader').find(\"fileDesc\").find(\"publicationStmt\").find(\"date\").text\n",
    "    return {\"wahlperiode\": wahlperiode, \"session\": session, \"date\": date}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Bundestagsreden des PolMine-Projektes wurden unter folgendem Link heruntergeladen: https://github.com/PolMine/GermaParlTEI\n",
    "\n",
    "Mit einem for-Loop werden die einzelnen Reden der xml-Dateien in einem dictionary abgespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_directory = \"GermaParlTEI-master\"\n",
    "corpus_dict = {}\n",
    "count = 0\n",
    "\n",
    "for path, dirs, files in os.walk(corpus_directory):\n",
    "    for index, file in enumerate(files):\n",
    "        xml_file = os.path.join(path, file)\n",
    "        meta_data_dict = get_meta_data(xml_file)\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for speaker in root.iter(\"sp\"):   \n",
    "            corpus_dict[count] = {}\n",
    "            corpus_dict[count][\"name\"] = speaker.get(\"name\")\n",
    "            corpus_dict[count][\"role\"] = speaker.get(\"role\")\n",
    "            corpus_dict[count][\"party\"] = speaker.get(\"party\")\n",
    "            corpus_dict[count][\"text\"] = \" \".join([paragraph.text for paragraph in speaker.iter(\"p\")])\n",
    "            corpus_dict[count][\"wahlperiode\"] = meta_data_dict[\"wahlperiode\"]\n",
    "            corpus_dict[count][\"session\"] = meta_data_dict[\"session\"]\n",
    "            corpus_dict[count][\"date\"] = meta_data_dict[\"date\"]\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Ländernamen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_country_names = [\"bosnien und herzegowina\", \"burkina faso\", \"costa rica\",\"dominikanische republik\",\n",
    "                      \"san marino\",\"saudi arabien\",\"sri lanka\",\"vereinigte arabische emirate\",\n",
    "                      \"sierra leone\", \"trinidad und tobago\", \"zentralafrikanische republik\",\n",
    "                      \"demokratische republik kongo\",\"vereinigtes königreich\"]   \n",
    "\n",
    "countries_names_german = []\n",
    "\n",
    "with open(\"Ländernamen.csv\", encoding=\"utf-8-sig\") as read_file:\n",
    "    csv_file = csv.reader(read_file)\n",
    "    for row in csv_file:\n",
    "        countries_names_german.append(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainieren der Wortvektormodelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasse um ein Wortvektomodell aus allen Bundestagsreden zu bilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences_Full():\n",
    "    \n",
    "    def __init__(self,corpus_dict):\n",
    "        self.corpus_dict = corpus_dict\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for speech in self.corpus_dict:\n",
    "            if self.corpus_dict[speech][\"role\"] == \"presidency\":\n",
    "                continue\n",
    "            speech_text = self.corpus_dict[speech][\"text\"]\n",
    "            speech_text = re.sub(\"[,'—–]\",\" \",speech_text)\n",
    "            speech_text = re.sub(\"-\",\"\",speech_text)\n",
    "            speech_text = speech_text.lower()\n",
    "            speech_text = speech_text.replace(\"saudiarabien\",\"saudi_arabien\")\n",
    "            speech_text = speech_text.replace(\"vereinigte staaten von amerika\",\"usa\")\n",
    "            speech_text = speech_text.replace(\"vereinigten staaten von amerika\",\"usa\")\n",
    "            speech_text = speech_text.replace(\"vereinigte staaten\",\"usa\")\n",
    "            speech_text = speech_text.replace(\"vereinigten staaten\",\"usa\")\n",
    "            speech_text = speech_text.replace(\"amerika\",\"usa\")\n",
    "            speech_text = speech_text.replace(\"botswana\",\"botsuana\")\n",
    "            speech_text = speech_text.replace(\"vereinigte königreich\",\"vereinigtes_königreich\")\n",
    "            speech_text = speech_text.replace(\"vereinigten königreichs\",\"vereinigtes_königreich\")\n",
    "            speech_text = speech_text.replace(\"großbritannien\",\"vereinigtes_königreich\")\n",
    "            for country in long_country_names:\n",
    "                speech_text = speech_text.replace(country, country.replace(\" \",\"_\"))\n",
    "            for country in countries_names_german:\n",
    "                speech_text = speech_text.replace(country.lower() + \"s\", country.lower())\n",
    "            final_text = [nltk.tokenize.word_tokenize(sentence) for sentence in nltk.tokenize.sent_tokenize(speech_text.lower())]\n",
    "            final_text = [[re.sub(\"[-,.'!?;–:—\\\"„“/]\",\"\",word) for word in sentence if re.sub(\"[-,.'!?;–:—\\\"„“/]\",\"\",word)] for sentence in final_text]\n",
    "            for sentence in final_text:\n",
    "                if sentence:\n",
    "                    yield sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasse um ein Wortvektormodell aus Bundestagsreden eines bestimmten Zeitraums zu bilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences_Period():\n",
    "    \n",
    "    def __init__(self,corpus_dict, year):\n",
    "        self.corpus_dict = corpus_dict\n",
    "        self.year = year\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for speech in self.corpus_dict:\n",
    "            speech_date = self.corpus_dict[speech][\"date\"]\n",
    "            speech_date = datetime.strptime(speech_date,\"%Y-%m-%d\")\n",
    "            if speech_date.year < self.year - 3 or speech_date.year > self.year:\n",
    "                continue\n",
    "            if self.corpus_dict[speech][\"role\"] == \"presidency\":\n",
    "                continue\n",
    "            speech_text = self.corpus_dict[speech][\"text\"]\n",
    "            speech_text = re.sub(\"[,'—–]\",\" \",speech_text)\n",
    "            speech_text = re.sub(\"-\",\"\",speech_text)\n",
    "            speech_text = speech_text.lower()\n",
    "            speech_text = speech_text.replace(\"saudiarabien\",\"saudi_arabien\")\n",
    "            speech_text = speech_text.replace(\"vereinigte staaten von amerika\",\"usa\")\n",
    "            speech_text = speech_text.replace(\"vereinigten staaten von amerika\",\"usa\")\n",
    "            speech_text = speech_text.replace(\"vereinigte staaten\",\"usa\")\n",
    "            speech_text = speech_text.replace(\"vereinigten staaten\",\"usa\")\n",
    "            speech_text = speech_text.replace(\"amerika\",\"usa\")\n",
    "            speech_text = speech_text.replace(\"botswana\",\"botsuana\")\n",
    "            speech_text = speech_text.replace(\"vereinigte königreich\",\"vereinigtes_königreich\")\n",
    "            speech_text = speech_text.replace(\"vereinigten königreichs\",\"vereinigtes_königreich\")\n",
    "            speech_text = speech_text.replace(\"großbritannien\",\"vereinigtes_königreich\")\n",
    "            for country in long_country_names:\n",
    "                speech_text = speech_text.replace(country, country.replace(\" \",\"_\"))\n",
    "            for country in countries_names_german:\n",
    "                speech_text = speech_text.replace(country.lower() + \"s\", country.lower())\n",
    "            final_text = [nltk.tokenize.word_tokenize(sentence) for sentence in nltk.tokenize.sent_tokenize(speech_text.lower())]\n",
    "            final_text = [[re.sub(\"[-,.'!?;–:—\\\"„“/]\",\"\",word) for word in sentence if re.sub(\"[-,.'!?;–:—\\\"„“/]\",\"\",word)] for sentence in final_text]\n",
    "            for sentence in final_text:\n",
    "                if sentence:\n",
    "                    yield sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainieren des vollständigen Wortvektormodells für alle Bundestagsreden zwischen 1996-2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences_Full(corpus_dict)\n",
    "full_model = gensim.models.Word2Vec(sentences,size=300, iter=15,window=15, sg=1)\n",
    "directory_path = pathlib.Path(\"Word2vec\")\n",
    "directory_path.mkdir(parents=True, exist_ok=True)\n",
    "model_name = pathlib.Path('word2vec_300dims_iter30_window15_skipgram_full_model')\n",
    "full_model.save(directory_path / model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainieren einzelner Wortvekormodelle für Bundestagsreden vierjähriger Zeiträume.  Für jeden Zeitraum werden 25 Modelle anhand unterschiedlicher, durch bootstrapping veränderter Datensätze gebildet. Jedes Modell wird mittels des Modells des vorherigen Zeitraumes initialisiert. Das erste Modell wird mittels des vollständigen Modells initialisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(1999,2017):\n",
    "    year_path = pathlib.Path(r\"Word2vec/Bootstrapping_Chronological/{}\".format(year))\n",
    "    year_path.mkdir(parents=True, exist_ok=True)\n",
    "    if year == 1999:\n",
    "        sentences = [sentence for sentence in MySentences_Period(corpus_dict, year)]\n",
    "        model = gensim.models.Word2Vec.load(r\"Word2vec/word2vec_300dims_iter30_window15_skipgram_full_model\")\n",
    "        model.train(sentences, total_examples=len(sentences),epochs=15)\n",
    "        model_name = pathlib.Path('word2vec_300dims_iter15_window15_skipgram_timespan{}-{}_fixedModel'.format(year-3,year))\n",
    "        model.save(str(year_path / model_name) )\n",
    "        for bootstrap_iteration in range(0,25):\n",
    "            bootstrap_sample = resample(sentences, replace=True, n_samples=None)\n",
    "            model = gensim.models.Word2Vec.load(r\"Word2vec/word2vec_300dims_iter30_window15_skipgram_full_model\")\n",
    "            model.train(bootstrap_sample, total_examples= len(bootstrap_sample),epochs=15)\n",
    "            model_name = pathlib.Path('word2vec_300dims_iter15_window15_skipgram_timespan{}-{}_bootstrap_iteration{}'.format(year-3,year,bootstrap_iteration))\n",
    "            model.save(str(year_path / model_name ))\n",
    "            print(\"Timespan:{}-{}  Bootstrap_iteration:{}  Corpus Count: {}\".format(year-3, year, bootstrap_iteration, model.corpus_count))\n",
    "    else:\n",
    "        prev_year_path = pathlib.Path(r\"Word2vec/Bootstrapping_Chronological/{}\".format(year-1))\n",
    "        prev_fixed_model_name = pathlib.Path('word2vec_300dims_iter15_window15_skipgram_timespan{}-{}_fixedModel'.format(year-4,year-1))\n",
    "        sentences = [sentence for sentence in MySentences_Period(corpus_dict, year)]\n",
    "        model = gensim.models.Word2Vec.load(prev_year_path / prev_fixed_model_name)\n",
    "        model.train(sentences, total_examples=len(sentences),epochs=15)\n",
    "        model_name = pathlib.Path('word2vec_300dims_iter15_window15_skipgram_timespan{}-{}_fixedModel'.format(year-3,year))\n",
    "        model.save(str(year_path / model_name ))\n",
    "        for bootstrap_iteration in range(0,25):\n",
    "            bootstrap_sample = resample(sentences, replace=True, n_samples=None)\n",
    "            model = gensim.models.Word2Vec.load(main_path + str(year - 1) + \"\\\\\" + \"word2vec_300dims_iter15_window15_skipgram_timespan{}-{}_fixedModel\".format(year-4,year-1))\n",
    "            model.train(bootstrap_sample, total_examples= len(bootstrap_sample),epochs=15)\n",
    "            model_name = pathlib.Path('word2vec_300dims_iter15_window15_skipgram_timespan{}-{}_bootstrap_iteration{}'.format(year-3,year,bootstrap_iteration))\n",
    "            model.save(str(year_path / model_name ))\n",
    "            print(\"Timespan:{}-{}  Bootstrap_iteration:{}  Corpus Count: {}\".format(year-3, year, bootstrap_iteration, model.corpus_count))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_3.6] *",
   "language": "python",
   "name": "conda-env-python_3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
